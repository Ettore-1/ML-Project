{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Machine learning project</h1>\n",
    "<hr>\n",
    "<p>This Jupyter notebook resume all our programmation work on binary classification, the Banknote Authentication Dataset and the Kidney Disease Dataset.</p>\n",
    "<p>Students:<br>\n",
    "    <li>Ettoré Hidoux</li>\n",
    "    <li>Agathe Fernandes Machado</li>\n",
    "    <li>Yasmine Diouri</li>\n",
    "    <li>Clément Mathé</li></p>\n",
    "\n",
    "## Summary :\n",
    "\n",
    "<p>Part I : Data Cleaning (function preprocessing)</p>\n",
    "<p>Part II : Result evaluation (function accuracy)</p>\n",
    "<p>Part III : Models used<br>\n",
    "    <li> 1-cross-validation (function models_1cv)</li>\n",
    "    <li> N-cross-validation (function models_Ncv)</li>\n",
    "    <li> Model mean (function model_mean)</li></p>\n",
    "<p>Part IV : Global function (function finalFunction)</p>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I : Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part contains one fonction which allows us to clean the data (replace string values by boolean, replace NaN values by the median of the column) and to suppress data caracteristics if the correlation between two caracteristics is higher than 0.75. At the end of the function, we create two data : the training data and the test data by a split method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries: Standard ones\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rnd\n",
    "# Libraries: scikit learn for preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocessing(abcd_csv, clas, lp, sep): # made by Ettoré Hidoux and Agathe Fernandes Machado\n",
    "    #abcd_csv is data name\n",
    "    #clas is the class namme of the data\n",
    "    #lp is the number of a full row \n",
    "    #sep is the symbol use as separator for the data\n",
    "    \n",
    "    # Load the data: data_banknote_authentification\n",
    "    data = pd.read_csv(abcd_csv,sep=sep)\n",
    "    \n",
    "    # X/Y separation\n",
    "    # transform class column from string into boolean if it's necessary\n",
    "    if isinstance(data[clas][lp], str):\n",
    "        Y = np.multiply([data[clas]==data[clas][0]],1)[0]\n",
    "    else:\n",
    "        Y = data[clas]\n",
    "    data.drop(clas, 1, inplace=True)\n",
    "    \n",
    "    #transform data column from string into boolean if it's necessary\n",
    "    for c in data:\n",
    "        if isinstance(data[c][lp], str):\n",
    "            a = np.multiply([data[c] == data[c][lp]],1)\n",
    "            data.drop(c, 1, inplace=True)\n",
    "            data[c] = a[0]\n",
    "        data[c] = np.nan_to_num(data[c], copy=True, nan=data[c].median())\n",
    "    \n",
    "    # Correlation matrix of the data columns \n",
    "    corr_matrix = data.corr().abs()\n",
    "    high_corr_var=np.where(corr_matrix>0.75)\n",
    "    high_corr_var=np.array([(corr_matrix.columns[x],corr_matrix.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y])\n",
    "    \n",
    "    # Suppression of correlate columns\n",
    "    for i in range(len(high_corr_var)):\n",
    "        c = high_corr_var[i][0]\n",
    "        data.drop(c, 1, inplace=True)\n",
    "        \n",
    "    X = data\n",
    "    X = StandardScaler().fit_transform(X) #normalize our data\n",
    "    \n",
    "    # Creation of a dataset to train and another to test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "    return  x_train, x_test, y_train, y_test, X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we apply our function preprocessing on our two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data_set1\n",
    "x_train1, x_test1, y_train1, y_test1, X1, Y1 = preprocessing(\"data_banknote_authentication.csv\", \"class\", 4, \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data_set2\n",
    "x_train2, x_test2, y_train2, y_test2, X2, Y2 = preprocessing(\"kidney_disease.csv\", \"classification\", 4, ',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II : Result evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the evaluation part, we implement a function which gives us the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# the function that give the percentage of accuracy of the function (the same as .score()) \n",
    "def accuracy(y_predict, y_test): # made by Clément Mathé\n",
    "    return np.mean([y_predict==y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III : Models used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 1-cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we implement a function which applies to our dataset cleaned up with 1-cross-validation thanks to different Machine Learning methods of scikit learn. This function returns the method which corresponds to the highest score (model='best') or all  of the scores of our methods (model=\"scores\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Models import list\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier  \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# model could take the value \"best\" or \"scores\"\n",
    "def models_1cv(x_train, x_test, y_train, y_test, model): # made by Ettoré Hidoux and Clément Mathé\n",
    "    # Models list\n",
    "    models = [svm.SVC(kernel='linear'),\n",
    "              svm.SVC(kernel='poly', degree=2, gamma='auto'),\n",
    "              svm.SVC(kernel='rbf', gamma='auto'),\n",
    "              svm.SVC(kernel='sigmoid', gamma=1./150),\n",
    "              SGDClassifier(),\n",
    "              DecisionTreeClassifier(),\n",
    "              GaussianNB(),\n",
    "              RandomForestRegressor(n_estimators = 1000, random_state = 42),\n",
    "              MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=42),\n",
    "              LogisticRegression(random_state=0)]\n",
    "    # Models apply the train dataset list\n",
    "    models = [clf.fit(x_train, y_train) for clf in models]\n",
    "    # Models score list\n",
    "    scores = [clf.score(x_test,y_test) for clf in models]\n",
    "\n",
    "    # Models name list \n",
    "    titles = ['SVC with linear kernel',\n",
    "              'SVC with polynomial (degree 2) kernel',\n",
    "              'SVC with RBF kernel',\n",
    "              'SVC with sigmoid kernel',\n",
    "              'Stochastic Gradient Descent',\n",
    "              'Desicion Trees',\n",
    "              'Bayesien Network: Gnb',\n",
    "              'Random Forest',\n",
    "              'Neural Network',\n",
    "              'Probit model']\n",
    "    \n",
    "    if model == 'best':\n",
    "        # return the name and the score of the method that obtain the best result \n",
    "        return titles[scores.index(max(scores))], max(scores)\n",
    "    if model == 'scores':\n",
    "        # return the name and the score of all the method\n",
    "        return [(titles[i],scores[i]) for i in range(10)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we apply our function on our two datasets separated in a training part and a test part (given by the function preprocessing) to see the results given by the function score of scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SVC with RBF kernel', 0.9709090909090909)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_1cv(x_train1, x_test1, y_train1, y_test1, 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SVC with RBF kernel', 0.9875)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_1cv(x_train2, x_test2, y_train2, y_test2, 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SVC with linear kernel', 0.9090909090909091),\n",
       " ('SVC with polynomial (degree 2) kernel', 0.7236363636363636),\n",
       " ('SVC with RBF kernel', 0.9709090909090909),\n",
       " ('SVC with sigmoid kernel', 0.8909090909090909),\n",
       " ('Stochastic Gradient Descent', 0.92),\n",
       " ('Desicion Trees', 0.9563636363636364),\n",
       " ('Bayesien Network: Gnb', 0.8036363636363636),\n",
       " ('Random Forest', 0.9055074922855927),\n",
       " ('Neural Network', 0.9672727272727273),\n",
       " ('Probit model', 0.9054545454545454)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_1cv(x_train1, x_test1, y_train1, y_test1, 'scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) N-cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we implement a function which applies to our dataset cleaned up with N-cross-validation (N chosen by the user) thanks to different Machine Learning methods of scikit learn (the same one than in 1-cross-validation). This function returns the method which corresponds to the highest score (model='best') or all  of the scores of our methods (model=\"scores\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models import list\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# model could take the value \"best\" or \"scores\"\n",
    "def models_Ncv(X, Y, N, model): # made by Yasmine Diouri and Agathe Fernandes Machado\n",
    "    # Models list\n",
    "    models = [svm.SVC(kernel='linear'),\n",
    "              svm.SVC(kernel='poly', degree=2, gamma='auto'),\n",
    "              svm.SVC(kernel='rbf', gamma='auto'),\n",
    "              svm.SVC(kernel='sigmoid', gamma=1./150),\n",
    "              SGDClassifier(),\n",
    "              DecisionTreeClassifier(),\n",
    "              GaussianNB(),\n",
    "              RandomForestRegressor(n_estimators = 1000, random_state = 42),\n",
    "              MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=42),\n",
    "              LogisticRegression(random_state=0)]\n",
    "    # Models score list with a N-cross-validation (the method is try N times each)\n",
    "    scores = [np.mean(cross_val_score(clf, X, Y, cv=N)) for clf in models]\n",
    "    \n",
    "    # Models name list\n",
    "    titles = ['SVC with linear kernel',\n",
    "              'SVC with polynomial (degree 2) kernel',\n",
    "              'SVC with RBF kernel',\n",
    "              'SVC with sigmoid kernel',\n",
    "              'Stochastic Gradient Descent',\n",
    "              'Desicion Trees',\n",
    "              'Bayesien Network: Gnb',\n",
    "              'Random Forest',\n",
    "              'Neural Network',\n",
    "              'Probit model']\n",
    "    \n",
    "    if model == 'best':\n",
    "        # return the name and the score of the method that obtain the best result \n",
    "        return titles[scores.index(max(scores))], max(scores)\n",
    "    if model == 'scores':\n",
    "        # return the name and the score of all the method\n",
    "        return [(titles[i],scores[i]) for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we apply our function on our two datasets not separated this time because we use N-cross-validation (here with N=10) to see the results given by the function score of scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SVC with RBF kernel', 0.9832487041150959)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_Ncv(X1,Y1,10, 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SVC with RBF kernel', 0.9875)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_Ncv(X2,Y2,10, 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SVC with linear kernel', 0.9082196128213266),\n",
       " ('SVC with polynomial (degree 2) kernel', 0.7377975245953665),\n",
       " ('SVC with RBF kernel', 0.9832487041150959),\n",
       " ('SVC with sigmoid kernel', 0.8965672273352375),\n",
       " ('Stochastic Gradient Descent', 0.8936898339151593),\n",
       " ('Desicion Trees', 0.9628530625198349),\n",
       " ('Bayesien Network: Gnb', 0.8325187771077964),\n",
       " ('Random Forest', 0.0922050889032258),\n",
       " ('Neural Network', 0.9766952290278219),\n",
       " ('Probit model', 0.9067703374590078)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_Ncv(X1,Y1,10, 'scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Model Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we implement the same function than the one about 1-cross-validation but we add a method which makes the mean of all our Machine Learning methods used previously. This function returns the method which corresponds to the highest score (model='best') or all  of the scores of our methods (model=\"scores\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models import list\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def model_mean(x_train, x_test, y_train, y_test, model): # made by Ettoré Hidoux\n",
    "    # Models list\n",
    "    models = [svm.SVC(kernel='linear'),\n",
    "              svm.SVC(kernel='poly', degree=2, gamma='auto'),\n",
    "              svm.SVC(kernel='rbf', gamma='auto'),\n",
    "              svm.SVC(kernel='sigmoid', gamma=1./150),\n",
    "              SGDClassifier(),\n",
    "              DecisionTreeClassifier(),\n",
    "              GaussianNB(),\n",
    "              RandomForestRegressor(n_estimators = 1000, random_state = 42),\n",
    "              MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=42),\n",
    "              LogisticRegression(random_state=0)]\n",
    "    # Models apply the train dataset list\n",
    "    models = [clf.fit(x_train, y_train) for clf in models]\n",
    "    # Models predictions for the input x_test\n",
    "    predicts = [clf.predict(x_test) for clf in models]\n",
    "    # Models score list\n",
    "    scores = [clf.score(x_test,y_test) for clf in models]\n",
    "    # Models predictions mean \n",
    "    mean_predict = sum(p for p in predicts)/10\n",
    "    mean_predict = [np.round(mean_predict[i]) for i in range(len(mean_predict))]\n",
    "    # Add model mean score to scores\n",
    "    scores.append(accuracy(mean_predict, y_test))\n",
    "\n",
    "    # Models name list\n",
    "    titles = ['SVC with linear kernel',\n",
    "              'SVC with polynomial (degree 2) kernel',\n",
    "              'SVC with RBF kernel',\n",
    "              'SVC with sigmoid kernel',\n",
    "              'Stochastic Gradient Descent',\n",
    "              'Desicion Trees',\n",
    "              'Bayesien Network: Gnb',\n",
    "              'Random Forest',\n",
    "              'Neural Network',\n",
    "              'Probit model',\n",
    "              'Model Mean']\n",
    "\n",
    "    if model == 'best':\n",
    "        # return the name and the score of the method that obtain the best result \n",
    "        return titles[scores.index(max(scores))], max(scores)\n",
    "    if model == 'scores':\n",
    "        # return the name and the score of all the method\n",
    "        return [(titles[i],scores[i]) for i in range(11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we apply our function on our two datasets separated in a training part and a test part (given by the function preprocessing) to see the results given by the function score of scikit learn for our previous methods and for the model mean, we use the function accuracy defines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SVC with RBF kernel', 0.9709090909090909)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mean(x_train1, x_test1, y_train1, y_test1, 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Model Mean', 1.0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mean(x_train2, x_test2, y_train2, y_test2, 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SVC with linear kernel', 0.9090909090909091),\n",
       " ('SVC with polynomial (degree 2) kernel', 0.7236363636363636),\n",
       " ('SVC with RBF kernel', 0.9709090909090909),\n",
       " ('SVC with sigmoid kernel', 0.8909090909090909),\n",
       " ('Stochastic Gradient Descent', 0.8945454545454545),\n",
       " ('Desicion Trees', 0.9527272727272728),\n",
       " ('Bayesien Network: Gnb', 0.8036363636363636),\n",
       " ('Random Forest', 0.9055074922855927),\n",
       " ('Neural Network', 0.9672727272727273),\n",
       " ('Probit model', 0.9054545454545454),\n",
       " ('Model Mean', 0.9345454545454546)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mean(x_train1, x_test1, y_train1, y_test1, 'scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III : Global function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we implement our final function which can apply on any dataset with binary classification where the user can choose which model he wants to apply to his dataset (1-cross-validation with model mean or N-cross-validation). So in this function, we use the previous functions defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries: Standard ones\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rnd\n",
    "# Libraries: scikit learn for preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Models import list\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def finalFunction(abcd_csv, clas, lp, sep, N, model): # made by Yasmine Diouri\n",
    "    #abcd_csv is data name\n",
    "    #clas is the class namme of the data\n",
    "    #lp is the number of a full row \n",
    "    #sep is the symbol use as separator for the data\n",
    "    #N is how many cross validation we want to apply to the data, N > O\n",
    "    #model tells us if we want the best scores or all the scores of models\n",
    "    # Data cleaning \n",
    "    x_train, x_test, y_train, y_test, X, Y = preprocessing(abcd_csv, clas, lp, sep)\n",
    "    # Show the result \n",
    "    if N == 1: #1-cross-validation\n",
    "        return model_mean(x_train, x_test, y_train, y_test, model) #use of the model_mean function defined below\n",
    "    if N > 1: #N-cross-validation\n",
    "        return models_Ncv(X, Y, N, model) #use of the models_Ncv function defined below\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
